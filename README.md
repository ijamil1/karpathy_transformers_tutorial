In this repo, I'm working in a notebook that was initially created/shared from Andrej Karpathy and is the result of the work he did in this tutorial video: https://youtu.be/kCc8FmEb1nY?si=akN2jMhDbXfki3qL. I am working on exploring/extending the simple transformed model he built. This is an exercise for developing more knowledge about transformers and the architecture behind LLMs.


There were 2 extensions/modifications I made to AK's model. The first modification I made was regarding how to concatenate the context vectors across attention heads within a block. I am referring to the output of the normalize(tril(Q * tranpose(K))) * V, where Q, K, and V are query, key, and value matrices respectively. In AK's model, he concatenates the context vectors across attention heads within a block. I modify this to be an element-wise average of context vectors across attention heads within a block. I barely see any improvement in the validation loss after 5000 steps when I modify the model to average context vectors. The validation loss in the original model was 1.8104. In the modified model, it was 1.7961. At first I was a bit surpised there was such little improvement. Upon reflection, it's not too surprising. The neural net doesn't have additional information encoded when averaging the context vectors instead of concatenating them. 

The second modification I made was making the model larger. The original model had 4 layers and 4 attention heads. I created a model with 6 layers and 8 attention heads. Even though, there is ~500k more parameters in the model with 6 layers and 8 attn heads as compared to the model with 4 layers and 4 attention heads, the performance on the larger model is only marginally better. The validation loss on the larger model after 5000 training steps is 1.7768. In the smaller model, it is 1.7961. This was genuinely a bit surprising; it could be that I didn't train for long enough.
